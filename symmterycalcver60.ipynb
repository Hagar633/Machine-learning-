{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hagar633/Machine-learning-/blob/main/symmterycalcver60.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d3e4cae655a3433ca875f85ad2ceff87",
            "5250f11817e74fec82c8927b6843b965",
            "2e266f87d4b24db8a7388050491bef35",
            "0e680760719e4715872f6a08d1dc132c",
            "815a39cefabc4c87b816eba18013628c",
            "24310b6a6cae4059b7804d5adec632fc",
            "c02fbe2dc25d469286b47859149e6232",
            "49890e3460af4e78bdab6ff9f33cdd8e",
            "5c877bd1c8af4c4a83ce7db0feffddaa",
            "623b4adff74b4c31baea50c4fe097093",
            "5b1752fa485c44acac28e99d3f6ecff2"
          ]
        },
        "id": "8b56sEgd0kZd",
        "outputId": "33d7f724-6b08-406d-9e7a-4337be1f2425"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3e4cae655a3433ca875f85ad2ceff87"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/models/segformer/feature_extraction_segformer.py:30: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/models/segformer/feature_extraction_segformer.py:35: UserWarning: The following named arguments are not valid for `SegformerFeatureExtractor.__init__` and were ignored: 'feature_extractor_type', 'reduce_labels'\n",
            "  super().__init__(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing kid1.mp4 ...\n",
            "Total frames: 223\n",
            "kk\n",
            "distl: 0.09083159344003877\n",
            "distr: 0.07632005595279479\n",
            "distl: 0.07539587424058204\n",
            "distr: 0.0007468202407168168\n",
            "distl: 0.1181826402064672\n",
            "distr: 0.0002908070733553814\n",
            "distl: 0.0011920861527180415\n",
            "distr: 0.00022937498054309557\n",
            "distl: 0.07833989792605468\n",
            "distr: 0.0009704274385770041\n",
            "distl: 0.0658661511806885\n",
            "distr: 0.00031522206102896355\n",
            "distl: 0.0010932963329712833\n",
            "distr: 0.00019210976153871687\n",
            "distl: 0.0006449557356658088\n",
            "distr: 0.0009382363888654767\n",
            "distl: 0.00041471993383568334\n",
            "distr: 0.0005181422110154267\n",
            "distl: 0.09374882545934927\n",
            "distr: 0.0014563009666217488\n",
            "distl: 0.11175456810043195\n",
            "distr: 0.0008757518610900129\n",
            "distl: 0.0018542627951507742\n",
            "distr: 0.0006473880089835627\n",
            "distl: 5.259806911306342e-05\n",
            "distr: 0.00033797735950902297\n",
            "distl: 0.06560919416035561\n",
            "distr: 0.16075568296641277\n",
            "distl: 0.02664105090854463\n",
            "distr: 0.15389292582728656\n",
            "distl: 0.0033118871452829657\n",
            "distr: 0.0001640682716340658\n",
            "distl: 0.10612682376680341\n",
            "distr: 0.0007451889833178795\n",
            "distl: 0.00010931081987686221\n",
            "distr: 0.009379728331386674\n",
            "distl: 3.089959262270182e-05\n",
            "distr: 0.15584532073179194\n",
            "distl: 0.03010569949328168\n",
            "distr: 9.960849430279417e-05\n",
            "distl: 0.0008408460726248013\n",
            "distr: 5.800406380851442e-05\n",
            "distl: 0.001061272219847198\n",
            "distr: 0.0011637921205817081\n",
            "distl: 0.0007657494441341006\n",
            "distr: 0.0031036441385260405\n",
            "distl: 0.0014232651039500777\n",
            "distr: 0.00048041244381780146\n",
            "distl: 0.003949221116376522\n",
            "distr: 0.00037744269938499997\n",
            "distl: 0.15661427253042784\n",
            "distr: 0.0003640242545679407\n",
            "distl: 0.00016378708700881542\n",
            "distr: 0.0002336512175107331\n",
            "distl: 0.00015491678191981162\n",
            "distr: 0.0025578157827124137\n",
            "distl: 2.4450467024372813e-05\n",
            "distr: 0.00045079218405463806\n",
            "distl: 0.0017407497349681833\n",
            "distr: 6.84405254391951e-05\n",
            "distl: 0.16338629850997236\n",
            "distr: 8.924736488180597e-05\n",
            "distl: 0.0001606636846593901\n",
            "distr: 0.003180855542220918\n",
            "{'symmetry': 0.333}\n",
            "Saved result to /content/drive/MyDrive/testff/kid1_fin.mp4\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from collections import deque\n",
        "import torch\n",
        "import depth_pro\n",
        "model, transform = depth_pro.create_model_and_transforms()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device).eval()\n",
        "model = model.to(device).eval()\n",
        "from pickle import FRAME\n",
        "from google.colab.patches import cv2_imshow\n",
        "from decord import VideoReader\n",
        "import mediapipe as mp\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import statistics\n",
        "import math\n",
        "import os\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "from sklearn.linear_model import RANSACRegressor\n",
        "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_id = \"nvidia/segformer-b0-finetuned-ade-512-512\"\n",
        "feature_extractor = SegformerFeatureExtractor.from_pretrained(model_id)\n",
        "\n",
        "model1 = SegformerForSemanticSegmentation.from_pretrained(model_id).to(device).eval()\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "def approximate_intrinsics(width, height):\n",
        "    c_x, c_y = width/2, height/2\n",
        "    return float(c_x), float(c_y)\n",
        "\n",
        "\n",
        "def symmetry_calc(left, right):\n",
        "    si = abs(left - right) / (0.5 * (left + right))\n",
        "\n",
        "    return {\n",
        "        \"symmetry\": round(si, 3)\n",
        "    }\n",
        "\n",
        "def project_point_to_plane(point, plane):\n",
        "    a, b, c, d = plane\n",
        "    x0, y0, z0 = point\n",
        "    normal = np.array([a, b, c], dtype=float)\n",
        "    normal /= np.linalg.norm(normal)  # normalize\n",
        "\n",
        "    # signed distance\n",
        "    dist = (a*x0 + b*y0 + c*z0 + d) / np.linalg.norm([a,b,c])\n",
        "\n",
        "    # projection onto plane\n",
        "    projected = np.array([x0, y0, z0]) - dist * normal\n",
        "    return projected, abs(dist)\n",
        "\n",
        "def safe_point_to_plane_distance(point, plane_model):\n",
        "    \"\"\"\n",
        "    Robust version of point-to-plane distance.\n",
        "    Handles None planes and normalizes.\n",
        "    \"\"\"\n",
        "    if plane_model is None:\n",
        "        return None\n",
        "    a, b, c, d = plane_model\n",
        "    X, Y, Z = point\n",
        "    denom = np.sqrt(a**2 + b**2 + c**2)\n",
        "    if denom < 1e-6:\n",
        "        return None\n",
        "    return abs(a*X + b*Y + c*Z + d) / denom\n",
        "\n",
        "def pixel_to_camera_coords(x, y, Z, f_x, f_y, c_x, c_y):\n",
        "   X = (x - c_x) * Z / f_x\n",
        "   Y = (y - c_y) * Z / f_y\n",
        "   return X, Y, Z\n",
        "\n",
        "def camera_to_pixel_coords(X, Y, Z, f_x, f_y, c_x, c_y):\n",
        "    u = int((X * f_x) / Z + c_x)\n",
        "    v = int((Y * f_y) / Z + c_y)\n",
        "    return u, v\n",
        "\n",
        "def point_to_plane_distance(point, plane_model):\n",
        "    a, b, c, d = plane_model\n",
        "    X, Y, Z = point\n",
        "    return abs(a*X + b*Y + c*Z + d) / np.sqrt(a**2 + b**2 + c**2)\n",
        "\n",
        "def median_depth_patch(depth_map, u, v, radius=3):\n",
        "    \"\"\"Return median depth around pixel (u,v). radius in pixels.\"\"\"\n",
        "    h, w = depth_map.shape\n",
        "    u0 = max(0, u-radius); u1 = min(w-1, u+radius)\n",
        "    v0 = max(0, v-radius); v1 = min(h-1, v+radius)\n",
        "    patch = depth_map[v0:v1+1, u0:u1+1].ravel()\n",
        "    patch = patch[np.isfinite(patch) & (patch > 0)]\n",
        "    if len(patch)==0:\n",
        "        return None\n",
        "    return float(np.median(patch))\n",
        "\n",
        "def fit_local_ground_plane_3d(depth_map, ground_mask, fx, fy, cx, cy,\n",
        "                              u_center, v_center, window=80, sample_step=4, min_pts=50):\n",
        "    \"\"\"\n",
        "    Fit plane in camera coordinates using ground_mask *near* (u_center,v_center).\n",
        "    Returns plane (a,b,c,d) for ax + by + cz + d = 0 and list of sampled 3D points used.\n",
        "    \"\"\"\n",
        "    h, w = depth_map.shape\n",
        "    u0 = max(0, int(u_center - window))\n",
        "    u1 = min(w-1, int(u_center + window))\n",
        "    v0 = max(0, int(v_center - window))\n",
        "    v1 = min(h-1, int(v_center + window))\n",
        "\n",
        "    us, vs = np.meshgrid(np.arange(u0, u1+1, sample_step),\n",
        "                         np.arange(v0, v1+1, sample_step))\n",
        "    us = us.ravel(); vs = vs.ravel()\n",
        "\n",
        "    mask_vals = ground_mask[vs, us]\n",
        "    zs = depth_map[vs, us]\n",
        "    keep = mask_vals & np.isfinite(zs) & (zs > 0.01)\n",
        "    if keep.sum() < min_pts:\n",
        "        return None, None  # not enough local ground points\n",
        "\n",
        "    us_k = us[keep]; vs_k = vs[keep]; zs_k = zs[keep]\n",
        "    # backproject to camera coords\n",
        "    Xs = (us_k - cx) * zs_k / fx\n",
        "    Ys = (vs_k - cy) * zs_k / fy\n",
        "    Zs = zs_k\n",
        "    pts3 = np.vstack([Xs, Ys, Zs]).T\n",
        "\n",
        "    # fit Z = a*X + b*Y + c  via RANSAC (robust)\n",
        "    XY = pts3[:, :2]\n",
        "    Zvals = pts3[:, 2]\n",
        "    ransac = RANSACRegressor(residual_threshold=0.03, min_samples=20, max_trials=200)\n",
        "    ransac.fit(XY, Zvals)\n",
        "    a, b = ransac.estimator_.coef_\n",
        "    c = float(ransac.estimator_.intercept_)\n",
        "    # plane: Z = aX + bY + c  ->  aX + bY - Z + c = 0\n",
        "    # get normalized (A,B,C,D)ر\n",
        "    A, B, C, D = a, b, -1.0, c\n",
        "    norm = np.array([A, B, C], dtype=float)\n",
        "    norm /= np.linalg.norm(norm)\n",
        "    D = D / np.linalg.norm([A,B,C])\n",
        "    return (norm[0], norm[1], norm[2], D), pts3\n",
        "\n",
        "def distance_3d(p1, p2):\n",
        "    return math.sqrt(\n",
        "        (p1[0] - p2[0])**2 +\n",
        "        (p1[1] - p2[1])**2 +\n",
        "        (p1[2] - p2[2])**2\n",
        "    )\n",
        "\n",
        "def support_polygon_decision(points, plane=None):\n",
        "    # points contains left_hip, right_hip, left_ankle, right_ankle\n",
        "\n",
        "    # 1. Mid-hip point = approximate CoM\n",
        "    com = (\n",
        "        (points[\"left_hip\"][0] + points[\"right_hip\"][0]) / 2,\n",
        "        (points[\"left_hip\"][1] + points[\"right_hip\"][1]) / 2,\n",
        "        (points[\"left_hip\"][2] + points[\"right_hip\"][2]) / 2,\n",
        "    )\n",
        "\n",
        "    if plane is not None:\n",
        "        # 2a. Project CoM vertically onto ground plane\n",
        "        com_proj, _ = project_point_to_plane(com, plane)\n",
        "    else:\n",
        "        # 2b. If no plane: use 2D (just keep x)\n",
        "        com_proj = com\n",
        "\n",
        "    # 3. Compare horizontal distances to feet\n",
        "    left_dist = np.linalg.norm(np.array(com_proj[:2]) - np.array(points[\"left_foot\"][:2]))\n",
        "    right_dist = np.linalg.norm(np.array(com_proj[:2]) - np.array(points[\"right_foot\"][:2]))\n",
        "\n",
        "    if left_dist < right_dist * 0.8:  # significantly closer\n",
        "        return 0\n",
        "    elif right_dist < left_dist * 0.8:\n",
        "        return 1\n",
        "    else:\n",
        "        return 2\n",
        "\n",
        "def angle_between_points(p0, p1, p2):\n",
        "    \"\"\"\n",
        "    Calculate the angle (in radians) at p0 formed by p1 and p2.\n",
        "    \"\"\"\n",
        "    v1 = np.array(p1) - np.array(p0)\n",
        "    v2 = np.array(p2) - np.array(p0)\n",
        "\n",
        "    dot = np.dot(v1, v2)\n",
        "    norm_product = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
        "\n",
        "    if norm_product == 0:\n",
        "        return None  # avoid division by zero\n",
        "\n",
        "    cos_theta = dot / norm_product\n",
        "    # Clamp value to [-1, 1] to avoid floating point errors\n",
        "    cos_theta = np.clip(cos_theta, -1.0, 1.0)\n",
        "\n",
        "    return np.degrees(np.arccos(cos_theta))\n",
        "\n",
        "\n",
        "def test_l_r(i,frame,ground_mask,depth_map,fx,fy,c_x,c_y,results,width,height):\n",
        "  right= False\n",
        "  left= False\n",
        "\n",
        "  plane, _ = fit_local_ground_plane_3d(depth_map, ground_mask, fx, fy, c_x, c_y,\n",
        "                                     u_center=width//2, v_center=height-50,\n",
        "                                     window=300, sample_step=4)\n",
        "  if plane is not None:\n",
        "     a, b, c, d = plane\n",
        "  step = 20  # density of grid (adjust)\n",
        "  for v in range(0, frame.shape[0], step):\n",
        "      for u in range(0, frame.shape[1], step):\n",
        "              Z = depth_map[v, u]\n",
        "              if Z <= 0:\n",
        "                      continue\n",
        "              X, Y, Z = pixel_to_camera_coords(u, v, Z, fx, fy, c_x, c_y)\n",
        "\n",
        "        # Check if point is near plane\n",
        "              dist = abs(a * X + b * Y +c * Z + d) / np.sqrt(a**2 + b**2 + c**2)\n",
        "              if dist < 0.05:  # 5cm tolerance to mark as \"on plane\"\n",
        "                 cv2.circle(frame, (u, v), 2, (0, 255, 0), -1)\n",
        "  points = {}\n",
        "  mp_pose = mp.solutions.pose\n",
        "  h, w, _ = frame.shape\n",
        "  if results.pose_landmarks:\n",
        "           for id, lm in enumerate(results.pose_landmarks.landmark):\n",
        "             cx= min(max(int(lm.x * w), 0), w - 1)\n",
        "             cy= min(max(int(lm.y * h), 0), h - 1)\n",
        "             cz = depth_map[cy, cx]\n",
        "\n",
        "             X, Y, Z = pixel_to_camera_coords(cx, cy, cz, fx, fy, c_x, c_y)\n",
        "             if id == mp_pose.PoseLandmark.LEFT_ANKLE.value:\n",
        "                points[\"left_ankle\"] = (X, Y, Z)\n",
        "             elif id == mp_pose.PoseLandmark.RIGHT_ANKLE.value:\n",
        "                points[\"right_ankle\"] = (X, Y, Z)\n",
        "             elif id == mp_pose.PoseLandmark.RIGHT_KNEE.value:\n",
        "                points[\"right_knee\"] = (X, Y, Z)\n",
        "             elif id == mp_pose.PoseLandmark.LEFT_KNEE.value:\n",
        "                points[\"left_knee\"] = (X, Y, Z)\n",
        "             elif id == mp_pose.PoseLandmark.RIGHT_HIP.value:\n",
        "                points[\"right_hip\"] = (X, Y, Z)\n",
        "             elif id == mp_pose.PoseLandmark.LEFT_HIP.value:\n",
        "                points[\"left_hip\"] = (X, Y, Z)\n",
        "             elif id== mp_pose.PoseLandmark.LEFT_FOOT_INDEX.value:\n",
        "                points[\"left_foot\"] = (X, Y, Z)\n",
        "             elif id == mp_pose.PoseLandmark.RIGHT_FOOT_INDEX.value:\n",
        "                points[\"right_foot\"] = (X, Y, Z)\n",
        "           disttr= distance_3d(points[\"right_ankle\"], points[\"right_hip\"])\n",
        "           disttl= distance_3d(points[\"left_ankle\"], points[\"left_hip\"])\n",
        "           angle_r= angle_between_points(points[\"right_knee\"], points[\"right_hip\"], points[\"right_ankle\"])\n",
        "           angle_l = angle_between_points(points[\"left_knee\"], points[\"left_hip\"], points[\"left_ankle\"])\n",
        "\n",
        "           if all(k in points for k in [\"left_hip\", \"left_knee\", \"left_ankle\"]):\n",
        "\n",
        "                u_hip, v_hip = camera_to_pixel_coords(*points[\"left_hip\"], fx, fy, c_x, c_y)\n",
        "                u_knee, v_knee = camera_to_pixel_coords(*points[\"left_knee\"], fx, fy, c_x, c_y)\n",
        "                u_ankle, v_ankle = camera_to_pixel_coords(*points[\"left_ankle\"], fx, fy, c_x, c_y)\n",
        "\n",
        "                cv2.line(frame, (u_hip, v_hip), (u_knee, v_knee), (0, 255, 0), 2)\n",
        "                cv2.line(frame, (u_knee, v_knee), (u_ankle, v_ankle), (0, 255, 0), 2)\n",
        "\n",
        "           if all(k in points for k in [\"right_hip\", \"right_knee\", \"right_ankle\"]):\n",
        "              u_hip, v_hip = camera_to_pixel_coords(*points[\"right_hip\"], fx, fy, c_x, c_y)\n",
        "              u_knee, v_knee = camera_to_pixel_coords(*points[\"right_knee\"], fx, fy, c_x, c_y)\n",
        "              u_ankle, v_ankle = camera_to_pixel_coords(*points[\"right_ankle\"], fx, fy, c_x, c_y)\n",
        "\n",
        "\n",
        "              cv2.line(frame, (u_hip, v_hip), (u_knee, v_knee), (255, 0, 0), 2)  # blue\n",
        "              cv2.line(frame, (u_knee, v_knee), (u_ankle, v_ankle), (255, 0, 0), 2)\n",
        "\n",
        "\n",
        "           distl = safe_point_to_plane_distance(points.get(\"left_ankle\"), plane)\n",
        "           distr = safe_point_to_plane_distance(points.get(\"right_ankle\"), plane)\n",
        "           print(f\"distl: {distl}\")\n",
        "           print(f\"distr: {distr}\")\n",
        "\n",
        "           proj_left, dist_left = project_point_to_plane(points[\"left_ankle\"], plane)\n",
        "\n",
        "           u1, v1 = camera_to_pixel_coords(*points[\"left_ankle\"], fx, fy, cx, cy)\n",
        "           u2, v2 = camera_to_pixel_coords(*proj_left, fx, fy, cx, cy)\n",
        "           cv2.line(frame, (u1, v1), (u2, v2), (0, 255, 0), 2)\n",
        "\n",
        "           cv2.line(frame, (u1, v1), (u2, v2), (0, 255, 0), 2)   # green line\n",
        "           cv2.circle(frame, (u1, v1), 5, (0, 0, 255), -1)       # ankle (red)\n",
        "           cv2.circle(frame, (u2, v2), 5, (255, 0, 0), -1)       # projection (blue)\n",
        "           cv2.putText(frame, f\"{dist_left:.3f} m\", (u1+10, v1-10),\n",
        "           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
        "\n",
        "           ankle_3d = points[\"right_ankle\"]\n",
        "           proj_right, dist_right = project_point_to_plane(ankle_3d, plane)\n",
        "\n",
        "           u1, v1 = camera_to_pixel_coords(*ankle_3d, fx, fy, cx, cy)\n",
        "           u2, v2 = camera_to_pixel_coords(*proj_right, fx, fy, cx, cy)\n",
        "\n",
        "           cv2.line(frame, (u1, v1), (u2, v2), (255, 255, 0), 2) # cyan line\n",
        "           cv2.circle(frame, (u1, v1), 5, (0, 0, 255), -1)       # ankle\n",
        "           cv2.circle(frame, (u2, v2), 5, (0, 255, 255), -1)     # projection\n",
        "           cv2.putText(frame, f\"{dist_right:.3f} m\", (u1+10, v1-10),\n",
        "           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)\n",
        "\n",
        "\n",
        "           left_contact  = distl  is not None and distl  < distr\n",
        "           right_contact = distr  is not None and distr  < distl\n",
        "\n",
        "\n",
        "           if left_contact and distr-distl >= 0.001:\n",
        "              truth = \"left\"\n",
        "              left = True\n",
        "\n",
        "           elif right_contact and distl-distr >= 0.001:\n",
        "                 truth = \"right\"\n",
        "                 right = True\n",
        "\n",
        "           else:\n",
        "               truth = \"none\"\n",
        "\n",
        "\n",
        "           text = f\"Frame: {i} | touching: {truth}| angle_r: {angle_r:.4f}|angle_l:{angle_l:.4f}\"\n",
        "\n",
        "           font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "           font_scale = 2.0\n",
        "           thickness = 1\n",
        "           (text_w, text_h), baseline = cv2.getTextSize(text, font, font_scale, thickness)\n",
        "           if text_w > frame.shape[1] - 20:  # keep margin\n",
        "               font_scale = (frame.shape[1] - 10) / (0.5*text_w)\n",
        "           x = 10\n",
        "           y = 50 + text_h\n",
        "           cv2.putText(frame, text, (x, y), font, font_scale, (0, 0, 200), thickness, cv2.LINE_AA)\n",
        "           text2 = f\"distl: {distl:.3f} m | distr: {distr:.3f} m\"\n",
        "           y2 = y + text_h + 10  # 10px spacing below first line\n",
        "           cv2.putText(frame, text2, (x, y2), font, font_scale, (0, 100, 255), thickness, cv2.LINE_AA)\n",
        "\n",
        "  return right, left\n",
        "\n",
        "\n",
        "def processvideo(video_path, output_path,model):\n",
        "    left=0\n",
        "    right=0\n",
        "    left_prev = False\n",
        "    right_prev= False\n",
        "    S_I=0\n",
        "    vr = VideoReader(video_path)\n",
        "    frame_count = len(vr)\n",
        "    print(f\"Total frames: {frame_count}\")\n",
        "    fps = 7\n",
        "    step = fps * 1\n",
        "    mp_pose = mp.solutions.pose\n",
        "    pose = mp_pose.Pose(static_image_mode=True)\n",
        "    mp_draw = mp.solutions.drawing_utils\n",
        "    sample_frame = vr[0].asnumpy()\n",
        "    rgb = cv2.cvtColor(sample_frame , cv2.COLOR_BGR2RGB)\n",
        "    results = pose.process(rgb)\n",
        "    img1 = transform(rgb).unsqueeze(0).to(device)\n",
        "    height, width, _ = sample_frame.shape\n",
        "    c_x,c_y= approximate_intrinsics(width, height)\n",
        "    with torch.no_grad():\n",
        "            pred = model.infer(img1)\n",
        "            depth_map = pred[\"depth\"].squeeze().cpu().numpy()\n",
        "            depth_map = cv2.resize(depth_map, (sample_frame.shape[1], sample_frame.shape[0]))\n",
        "            intrinsics = pred[\"focallength_px\"].cpu().numpy()\n",
        "            if np.ndim(intrinsics) == 0:\n",
        "              fx = fy = float(intrinsics)\n",
        "            else:\n",
        "              fx, fy = intrinsics\n",
        "    print(\"kk\")\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    for i in range(0, frame_count, step):\n",
        "        frame = vr[i].asnumpy()\n",
        "        rgb = cv2.cvtColor(frame , cv2.COLOR_BGR2RGB)\n",
        "        results = pose.process(rgb)\n",
        "        #ectracting ground\n",
        "        img = transform(rgb).unsqueeze(0).to(device)\n",
        "        inputs = feature_extractor(rgb, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "             outputs = model1(**inputs)\n",
        "             seg = outputs.logits.argmax(dim=1).squeeze().cpu().numpy()\n",
        "             seg_resized = cv2.resize(seg.astype(np.uint8),\n",
        "                         (depth_map.shape[1], depth_map.shape[0]),\n",
        "                         interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        ground_ids = [3, 9, 13, 29, 94] # floor, grass, earth,field, land\n",
        "        ground_mask = np.isin(seg_resized, ground_ids)\n",
        "        cv2.imwrite(\"ground_mask.png\", ground_mask.astype(np.uint8) * 255)\n",
        "        right_t, left_t= test_l_r(i,frame,ground_mask,depth_map,fx,fy,c_x,c_y,results,width,height)\n",
        "\n",
        "        if right_t and left_t== False:\n",
        "          if right_prev == False:\n",
        "            right_prev=  True\n",
        "            left_prev= False\n",
        "            right+=1\n",
        "        elif left_t and right_t== False:\n",
        "          if left_prev == False:\n",
        "            left_prev= True\n",
        "            right_prev= False\n",
        "            left+=1\n",
        "        else:\n",
        "          right_prev= False\n",
        "          left_prev= False\n",
        "\n",
        "\n",
        "        out.write(frame)\n",
        "    if left > 0 or right !=0:\n",
        "       S_I= symmetry_calc(left,right)\n",
        "       print(S_I)\n",
        "\n",
        "    black_frame = np.zeros_like(frame)\n",
        "    for _ in range(fps * 3):\n",
        "\n",
        "         frame_with_text = black_frame.copy()\n",
        "\n",
        "         txt = f\"{S_I}\"\n",
        "         txt1 = f\"left: {left}\"\n",
        "         txt2 = f\"right: {right}\"\n",
        "\n",
        "         font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "         font_scale = 2.0\n",
        "         thickness = 3\n",
        "         color = (255, 255, 255)\n",
        "\n",
        "\n",
        "         (text_width, text_height), baseline = cv2.getTextSize(txt, font, font_scale, thickness)\n",
        "         (text_width1, text_height1), baseline = cv2.getTextSize(txt1, font, font_scale, thickness)\n",
        "         (text_width2, text_height2), baseline = cv2.getTextSize(txt2, font, font_scale, thickness)\n",
        "\n",
        "\n",
        "         x = (width - text_width) // 2\n",
        "         y = (height + text_height) // 2\n",
        "         x1 = (width - text_width1) // 2\n",
        "         y1 = (height + 4* text_height1) // 2\n",
        "         x2 = (width - text_width2) // 2\n",
        "         y2 = (height + 8* text_height2) // 2\n",
        "\n",
        "\n",
        "         cv2.putText(frame_with_text,\n",
        "            txt,\n",
        "            (x, y),\n",
        "            font,\n",
        "            font_scale,\n",
        "            color,\n",
        "            thickness,\n",
        "            cv2.LINE_AA)\n",
        "         cv2.putText(frame_with_text,\n",
        "            txt1,\n",
        "            (x1, y1),\n",
        "            font,\n",
        "            font_scale,\n",
        "            color,\n",
        "            thickness,\n",
        "            cv2.LINE_AA)\n",
        "         cv2.putText(frame_with_text,\n",
        "            txt2,\n",
        "            (x2, y2),\n",
        "            font,\n",
        "            font_scale,\n",
        "            color,\n",
        "            thickness,\n",
        "            cv2.LINE_AA)\n",
        "\n",
        "         out.write(frame_with_text)\n",
        "    out.release()\n",
        "\n",
        "\n",
        "input_folder = \"/content/drive/MyDrive/test\"\n",
        "output_folder = \"/content/drive/MyDrive/testff\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "video_files = [f for f in os.listdir(input_folder) if f.lower().endswith((\".mp4\", \".avi\", \".mov\"))]\n",
        "\n",
        "for video in video_files:\n",
        "    input_path = os.path.join(input_folder, video)\n",
        "    output_name = os.path.splitext(video)[0] + \"_fin.mp4\"\n",
        "    output_path = os.path.join(output_folder, output_name)\n",
        "\n",
        "    print(f\"Processing {video} ...\")\n",
        "    processvideo(input_path, output_path,model)\n",
        "    print(f\"Saved result to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_zCofO6kMUe",
        "outputId": "9bec3cc6-41d1-46cd-a4dd-feec37f2c566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/apple/ml-depth-pro.git\n",
            "  Cloning https://github.com/apple/ml-depth-pro.git to /tmp/pip-req-build-fdexg9gi\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/apple/ml-depth-pro.git /tmp/pip-req-build-fdexg9gi\n",
            "  Resolved https://github.com/apple/ml-depth-pro.git to commit 9efe5c1def37a26c5367a71df664b18e1306c708\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from depth_pro==0.1) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from depth_pro==0.1) (0.23.0+cu126)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (from depth_pro==0.1) (1.0.19)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.12/dist-packages (from depth_pro==0.1) (1.26.4)\n",
            "Collecting pillow_heif (from depth_pro==0.1)\n",
            "  Downloading pillow_heif-1.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from depth_pro==0.1) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->depth_pro==0.1) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->depth_pro==0.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->depth_pro==0.1) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->depth_pro==0.1) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->depth_pro==0.1) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->depth_pro==0.1) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->depth_pro==0.1) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->depth_pro==0.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm->depth_pro==0.1) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm->depth_pro==0.1) (0.35.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm->depth_pro==0.1) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->depth_pro==0.1) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->depth_pro==0.1) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->depth_pro==0.1) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->depth_pro==0.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->depth_pro==0.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->depth_pro==0.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->depth_pro==0.1) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->depth_pro==0.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->depth_pro==0.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->depth_pro==0.1) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->depth_pro==0.1) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->depth_pro==0.1) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->depth_pro==0.1) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->depth_pro==0.1) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->depth_pro==0.1) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->depth_pro==0.1) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->depth_pro==0.1) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->depth_pro==0.1) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->depth_pro==0.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->depth_pro==0.1) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->depth_pro==0.1) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->depth_pro==0.1) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->depth_pro==0.1) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->depth_pro==0.1) (1.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm->depth_pro==0.1) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm->depth_pro==0.1) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm->depth_pro==0.1) (1.1.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->depth_pro==0.1) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm->depth_pro==0.1) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm->depth_pro==0.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm->depth_pro==0.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm->depth_pro==0.1) (2025.8.3)\n",
            "Downloading pillow_heif-1.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: depth_pro\n",
            "  Building wheel for depth_pro (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for depth_pro: filename=depth_pro-0.1-py3-none-any.whl size=27593 sha256=da4b6edc482d64ab94766e604a9125d81c47bef56b23fc737d9548461e677d23\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8k008cvl/wheels/3f/ad/4e/a2b790772df19931cab8bf9c5af51665ae6502f39589e662c4\n",
            "Successfully built depth_pro\n",
            "Installing collected packages: pillow_heif, depth_pro\n",
            "Successfully installed depth_pro-0.1 pillow_heif-1.1.0\n",
            "--2025-09-29 17:39:22--  https://huggingface.co/apple/DepthPro/resolve/main/depth_pro.pt\n",
            "Resolving huggingface.co (huggingface.co)... 54.230.71.2, 54.230.71.56, 54.230.71.103, ...\n",
            "Connecting to huggingface.co (huggingface.co)|54.230.71.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/66feae111e0b212adcd8809d/c53d9191a99d9439cd808aa8982cffa8459bedfa0f358dddd1653d703f8dece9?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250929%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250929T173923Z&X-Amz-Expires=3600&X-Amz-Signature=04774e1aa821833e58e19f144f147c4f336e01e790d80d757820f5f95d410ba4&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27depth_pro.pt%3B+filename%3D%22depth_pro.pt%22%3B&x-id=GetObject&Expires=1759171163&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1OTE3MTE2M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NmZlYWUxMTFlMGIyMTJhZGNkODgwOWQvYzUzZDkxOTFhOTlkOTQzOWNkODA4YWE4OTgyY2ZmYTg0NTliZWRmYTBmMzU4ZGRkZDE2NTNkNzAzZjhkZWNlOSoifV19&Signature=EdgFDSh7XrwAxxev1z2y2qI9g3hosORMktn7qGMShBcwk-mVZWRH1p6kD2UqjGacawDb6uw1ZGIGsvOUKVlRsqH0XI6iaaksHI0Za1G7st0fAjIwIzMRhrlbnnqkP6Nme-zJleDWUo4XS9I2H7GjpKAz8X-nyZJMjK5BkTx92KVNFBbsRI-V932Bv4%7EJ-WmugbnI9vys7EhGLH%7EcYVO3gvRVt5fYnsXOJrlVYdi2V6TubTSjX%7ECQ59NDYQeG7XIg97pu0l1YcTBapsQLHhAdJAfF1CEY31KJb-vQHso-7Iw%7EJSB0vUA8qrBeOmhV1nbLyqWygCrCyT05Q3oYRFyGzQ__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-09-29 17:39:23--  https://cas-bridge.xethub.hf.co/xet-bridge-us/66feae111e0b212adcd8809d/c53d9191a99d9439cd808aa8982cffa8459bedfa0f358dddd1653d703f8dece9?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250929%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250929T173923Z&X-Amz-Expires=3600&X-Amz-Signature=04774e1aa821833e58e19f144f147c4f336e01e790d80d757820f5f95d410ba4&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27depth_pro.pt%3B+filename%3D%22depth_pro.pt%22%3B&x-id=GetObject&Expires=1759171163&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1OTE3MTE2M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NmZlYWUxMTFlMGIyMTJhZGNkODgwOWQvYzUzZDkxOTFhOTlkOTQzOWNkODA4YWE4OTgyY2ZmYTg0NTliZWRmYTBmMzU4ZGRkZDE2NTNkNzAzZjhkZWNlOSoifV19&Signature=EdgFDSh7XrwAxxev1z2y2qI9g3hosORMktn7qGMShBcwk-mVZWRH1p6kD2UqjGacawDb6uw1ZGIGsvOUKVlRsqH0XI6iaaksHI0Za1G7st0fAjIwIzMRhrlbnnqkP6Nme-zJleDWUo4XS9I2H7GjpKAz8X-nyZJMjK5BkTx92KVNFBbsRI-V932Bv4%7EJ-WmugbnI9vys7EhGLH%7EcYVO3gvRVt5fYnsXOJrlVYdi2V6TubTSjX%7ECQ59NDYQeG7XIg97pu0l1YcTBapsQLHhAdJAfF1CEY31KJb-vQHso-7Iw%7EJSB0vUA8qrBeOmhV1nbLyqWygCrCyT05Q3oYRFyGzQ__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 13.33.183.18, 13.33.183.37, 13.33.183.46, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|13.33.183.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1904446787 (1.8G)\n",
            "Saving to: ‘checkpoints/depth_pro.pt’\n",
            "\n",
            "checkpoints/depth_p 100%[===================>]   1.77G  21.7MB/s    in 85s     \n",
            "\n",
            "2025-09-29 17:40:48 (21.4 MB/s) - ‘checkpoints/depth_pro.pt’ saved [1904446787/1904446787]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install mediapipe\n",
        "!pip install decord\n",
        "\n",
        "\n",
        "!pip install git+https://github.com/apple/ml-depth-pro.git\n",
        "!mkdir -p checkpoints\n",
        "!wget https://huggingface.co/apple/DepthPro/resolve/main/depth_pro.pt -O checkpoints/depth_pro.pt"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPqaxdYu+o4nxYSR79TBBG0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d3e4cae655a3433ca875f85ad2ceff87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5250f11817e74fec82c8927b6843b965",
              "IPY_MODEL_2e266f87d4b24db8a7388050491bef35",
              "IPY_MODEL_0e680760719e4715872f6a08d1dc132c"
            ],
            "layout": "IPY_MODEL_815a39cefabc4c87b816eba18013628c"
          }
        },
        "5250f11817e74fec82c8927b6843b965": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24310b6a6cae4059b7804d5adec632fc",
            "placeholder": "​",
            "style": "IPY_MODEL_c02fbe2dc25d469286b47859149e6232",
            "value": "Fetching 1 files: 100%"
          }
        },
        "2e266f87d4b24db8a7388050491bef35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49890e3460af4e78bdab6ff9f33cdd8e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c877bd1c8af4c4a83ce7db0feffddaa",
            "value": 1
          }
        },
        "0e680760719e4715872f6a08d1dc132c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_623b4adff74b4c31baea50c4fe097093",
            "placeholder": "​",
            "style": "IPY_MODEL_5b1752fa485c44acac28e99d3f6ecff2",
            "value": " 1/1 [00:00&lt;00:00, 131.09it/s]"
          }
        },
        "815a39cefabc4c87b816eba18013628c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24310b6a6cae4059b7804d5adec632fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c02fbe2dc25d469286b47859149e6232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49890e3460af4e78bdab6ff9f33cdd8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c877bd1c8af4c4a83ce7db0feffddaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "623b4adff74b4c31baea50c4fe097093": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b1752fa485c44acac28e99d3f6ecff2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}